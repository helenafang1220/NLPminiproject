{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "265dafb1",
   "metadata": {},
   "source": [
    "### <p style=\"font-family:Garamond,serif\">This notebook is for scraping tweets. </p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45409b52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: snscrape in /Users/chenxifang/opt/anaconda3/lib/python3.8/site-packages (0.3.5.dev138+ga6b6f3f)\n",
      "Requirement already satisfied: requests[socks] in /Users/chenxifang/opt/anaconda3/lib/python3.8/site-packages (from snscrape) (2.25.1)\n",
      "Requirement already satisfied: lxml in /Users/chenxifang/opt/anaconda3/lib/python3.8/site-packages (from snscrape) (4.6.3)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/chenxifang/opt/anaconda3/lib/python3.8/site-packages (from snscrape) (4.9.3)\n",
      "Requirement already satisfied: pytz in /Users/chenxifang/opt/anaconda3/lib/python3.8/site-packages (from snscrape) (2021.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/chenxifang/opt/anaconda3/lib/python3.8/site-packages (from beautifulsoup4->snscrape) (2.2.1)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/chenxifang/opt/anaconda3/lib/python3.8/site-packages (from requests[socks]->snscrape) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/chenxifang/opt/anaconda3/lib/python3.8/site-packages (from requests[socks]->snscrape) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/chenxifang/opt/anaconda3/lib/python3.8/site-packages (from requests[socks]->snscrape) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/chenxifang/opt/anaconda3/lib/python3.8/site-packages (from requests[socks]->snscrape) (2.10)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /Users/chenxifang/opt/anaconda3/lib/python3.8/site-packages (from requests[socks]->snscrape) (1.7.1)\n"
     ]
    }
   ],
   "source": [
    "#Install the twitter scraping library\n",
    "!pip install snscrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "beff5fce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chenxifang/opt/anaconda3/lib/python3.8/site-packages/pugnlp/constants.py:136: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  [datetime.datetime, pd.datetime, pd.Timestamp])\n",
      "/Users/chenxifang/opt/anaconda3/lib/python3.8/site-packages/pugnlp/constants.py:158: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  MIN_TIMESTAMP = pd.Timestamp(pd.datetime(1677, 9, 22, 0, 12, 44), tz='utc')\n",
      "/Users/chenxifang/opt/anaconda3/lib/python3.8/site-packages/pugnlp/tutil.py:100: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  np = pd.np\n",
      "/Users/chenxifang/opt/anaconda3/lib/python3.8/site-packages/pugnlp/util.py:80: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  np = pd.np\n",
      "/Users/chenxifang/opt/anaconda3/lib/python3.8/site-packages/nlpia/futil.py:30: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  np = pd.np\n",
      "/Users/chenxifang/opt/anaconda3/lib/python3.8/site-packages/nlpia/loaders.py:78: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  np = pd.np\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "from sklearn.metrics.pairwise import cosine_similarity as cosine\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from nltk.tokenize.casual import casual_tokenize\n",
    "import pandas as pd\n",
    "from nlpia.data.loaders import get_data\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e685e3c5",
   "metadata": {},
   "source": [
    "### <p style=\"font-family:Garamond,serif\"> These code are from NLP course's notebook 4.1 Task. I'm using the sns scrape to scrape data from Twitter. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e962ff12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4de11694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initialise your data frame\n",
    "hashtag_pfizer_df = pd.DataFrame(columns = [\"text\", \"label\"])\n",
    "hashtag_pfizer_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7e69bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many to grab\n",
    "maxTweets = 100000\n",
    "\n",
    "###Comment out methods below as appropriate \n",
    "\n",
    "# ---EITHER--- Keyword search\n",
    "# keyword = 'data'\n",
    "# data = sntwitter.TwitterSearchScraper(keyword + '-filter:links -filter:replies').get_items()\n",
    "\n",
    "# ---OR--- User search\n",
    "#user = 'ASTRAZENECAUK'\n",
    "#data = sntwitter.TwitterSearchScraper('from:ASTRAZENECAUK').get_items()\n",
    "\n",
    "# ---OR--- Hastag search\n",
    "hashtag = '#pfizer'\n",
    "data = sntwitter.TwitterHashtagScraper(hashtag + '-filter:links -filter:replies').get_items()\n",
    "\n",
    "#Set the label for this group\n",
    "hashtag_pfizer = []\n",
    "#Collect in array and pair each tweet with a class label\n",
    "for i,tweet in enumerate(data):\n",
    "    if i >= maxTweets:\n",
    "        break   \n",
    "    hashtag_pfizer = hashtag_pfizer + [[tweet.content]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2e1c1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"['Het schijnt dat de tuinman van #Pfizer  inmiddels ook miljonair is \\\\n #nosjournaal']\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hashtag_pfizer)\n",
    "str(hashtag_pfizer[1])\n",
    "print(type(str(hashtag_pfizer[0]))) #make sure that the data type in the array are stings\n",
    "str(hashtag_pfizer[22]) #Check the tweets in the array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eece43c",
   "metadata": {},
   "source": [
    "### <p style=\"font-family:Garamond,serif\">I initially tried to use TextBlob to filter out English Tweets, however, the function has limitation on the amount of input data per-day, so I had to give up on this function.</p>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "09a9affe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from textblob import TextBlob, Word, Blobber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6da8c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U pip setuptools wheel\n",
    "!pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9d2d1a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm\n",
    "!pip install spacy-langdetect #intall spacy language detect to filer language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d91da6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_hashtag_pfizer = [] #create an array for storing English Tweets only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252618cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import spacy for language detection\n",
    "#code sourse : https://stackoverflow.com/questions/66712753/how-to-use-languagedetector-from-spacy-langdetect-package\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy_langdetect import LanguageDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f47393",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This bit of code only need to run once !\n",
    "def get_lang_detector(nlp, name):\n",
    "    return LanguageDetector()\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "Language.factory(\"language_detector\", func=get_lang_detector)\n",
    "nlp.add_pipe('language_detector', last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8b90063a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'language': 'pt', 'score': 0.9999961485552193}\n",
      "[['language' 'pt']\n",
      " ['score' '0.9999955735145803']]\n"
     ]
    }
   ],
   "source": [
    "text = str(hashtag_pfizer[2])\n",
    "doc = nlp(text)\n",
    "print(doc._.language)\n",
    "type(doc._.language)\n",
    "\n",
    "#turn dict into array \n",
    "lang_doc = list(dict.items(doc._.language))\n",
    "lang_array = np.array(lang_doc)\n",
    "\n",
    "print(lang_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0dbb310d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80104, 1)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(100000):\n",
    "    text = str(hashtag_pfizer[i])\n",
    "    doc = nlp(text)\n",
    "    doc._.language\n",
    "    lang_doc = list(dict.items(doc._.language))\n",
    "    lang_array = np.array(lang_doc)\n",
    "    #print(lang_array)\n",
    "    if lang_array[0][1] == 'en':\n",
    "        en_hashtag_pfizer.append(text)\n",
    "#print(en_lang)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326fadd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_hashtag_pfizer[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "febf50ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80104, 1)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_hashtag_pfizer_df = pd.DataFrame(en_hashtag_pfizer, columns = [\"text\"])\n",
    "en_hashtag_pfizer_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "115ed84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#>>>For cleaning dataframe if store wrong<<<\n",
    "#en_hashtag_pfizer_df.drop(en_hashtag_pfizer_df.index, inplace=True)\n",
    "#df_en_lang.drop(df_en_lang.loc[:80103].columns, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ac77cb5b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "\u001b[K     |████████████████████████████████| 981 kB 6.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /Users/chenxifang/opt/anaconda3/lib/python3.8/site-packages (from langdetect) (1.15.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993221 sha256=f3438c88683b2ad9e93ec1a9b22a6fdb99c43247b419a2a8258683be2e2cb08e\n",
      "  Stored in directory: /Users/chenxifang/Library/Caches/pip/wheels/13/c7/b0/79f66658626032e78fc1a83103690ef6797d551cb22e56e734\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.9\n"
     ]
    }
   ],
   "source": [
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ed2d21cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['I got my #Pfizer booster shot today and the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['If the U.S. would release the vaccine, deman...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[\"Happy to get my #Pfizer booster on Saturday ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['Is the U.S. Treasury heavy in #Pfizer which ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['They can always reverse it at a later date. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80099</th>\n",
       "      <td>['The two #Pfizer scientists responsible for t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80100</th>\n",
       "      <td>['Stock markets: the big fat #Coronarotation\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80101</th>\n",
       "      <td>['As I have long said, #Pfizer and the others ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80102</th>\n",
       "      <td>['Sure is interesting timing that the announce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80103</th>\n",
       "      <td>[\"#Pfizer waited for a few days after the elec...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80104 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text\n",
       "0      ['I got my #Pfizer booster shot today and the ...\n",
       "1      ['If the U.S. would release the vaccine, deman...\n",
       "2      [\"Happy to get my #Pfizer booster on Saturday ...\n",
       "3      ['Is the U.S. Treasury heavy in #Pfizer which ...\n",
       "4      ['They can always reverse it at a later date. ...\n",
       "...                                                  ...\n",
       "80099  ['The two #Pfizer scientists responsible for t...\n",
       "80100  ['Stock markets: the big fat #Coronarotation\\n...\n",
       "80101  ['As I have long said, #Pfizer and the others ...\n",
       "80102  ['Sure is interesting timing that the announce...\n",
       "80103  [\"#Pfizer waited for a few days after the elec...\n",
       "\n",
       "[80104 rows x 1 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_hashtag_pfizer_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d19d8654",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write to file\n",
    "file_name = \"en_hashtag_pfizer\"\n",
    "en_hashtag_pfizer_df.to_csv(\"data/\" + file_name + \".tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "2703f463",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read back in\n",
    "en_hashtag_pfizer= pd.read_csv(\"data/\" + file_name + \".tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bfcd3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
