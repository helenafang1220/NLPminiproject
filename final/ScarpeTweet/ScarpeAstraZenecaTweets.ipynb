{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "265dafb1",
   "metadata": {},
   "source": [
    "### <p style=\"font-family:Garamond,serif\">This notebook is for scraping tweets. </p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45409b52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: snscrape in /Users/chenxifang/opt/anaconda3/lib/python3.8/site-packages (0.3.5.dev138+ga6b6f3f)\n",
      "Requirement already satisfied: requests[socks] in /Users/chenxifang/opt/anaconda3/lib/python3.8/site-packages (from snscrape) (2.25.1)\n",
      "Requirement already satisfied: lxml in /Users/chenxifang/opt/anaconda3/lib/python3.8/site-packages (from snscrape) (4.6.3)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/chenxifang/opt/anaconda3/lib/python3.8/site-packages (from snscrape) (4.9.3)\n",
      "Requirement already satisfied: pytz in /Users/chenxifang/opt/anaconda3/lib/python3.8/site-packages (from snscrape) (2021.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/chenxifang/opt/anaconda3/lib/python3.8/site-packages (from beautifulsoup4->snscrape) (2.2.1)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/chenxifang/opt/anaconda3/lib/python3.8/site-packages (from requests[socks]->snscrape) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/chenxifang/opt/anaconda3/lib/python3.8/site-packages (from requests[socks]->snscrape) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/chenxifang/opt/anaconda3/lib/python3.8/site-packages (from requests[socks]->snscrape) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/chenxifang/opt/anaconda3/lib/python3.8/site-packages (from requests[socks]->snscrape) (2.10)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /Users/chenxifang/opt/anaconda3/lib/python3.8/site-packages (from requests[socks]->snscrape) (1.7.1)\n"
     ]
    }
   ],
   "source": [
    "#Install the twitter scraping library\n",
    "!pip install snscrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beff5fce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chenxifang/opt/anaconda3/lib/python3.8/site-packages/pugnlp/constants.py:136: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  [datetime.datetime, pd.datetime, pd.Timestamp])\n",
      "/Users/chenxifang/opt/anaconda3/lib/python3.8/site-packages/pugnlp/constants.py:158: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  MIN_TIMESTAMP = pd.Timestamp(pd.datetime(1677, 9, 22, 0, 12, 44), tz='utc')\n",
      "/Users/chenxifang/opt/anaconda3/lib/python3.8/site-packages/pugnlp/tutil.py:100: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  np = pd.np\n",
      "/Users/chenxifang/opt/anaconda3/lib/python3.8/site-packages/pugnlp/util.py:80: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  np = pd.np\n",
      "/Users/chenxifang/opt/anaconda3/lib/python3.8/site-packages/nlpia/futil.py:30: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  np = pd.np\n",
      "/Users/chenxifang/opt/anaconda3/lib/python3.8/site-packages/nlpia/loaders.py:78: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  np = pd.np\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "from sklearn.metrics.pairwise import cosine_similarity as cosine\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from nltk.tokenize.casual import casual_tokenize\n",
    "import pandas as pd\n",
    "from nlpia.data.loaders import get_data\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e685e3c5",
   "metadata": {},
   "source": [
    "### <p style=\"font-family:Garamond,serif\"> These code are from NLP course's notebook 4.1 Task. I'm using the sns scrape to scrape data from Twitter. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e962ff12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4de11694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initialise dataframe\n",
    "hashtag_az_df = pd.DataFrame(columns = [\"text\"])\n",
    "hashtag_az_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7e69bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many to grab\n",
    "maxTweets = 100000\n",
    "\n",
    "###Comment out methods below as appropriate \n",
    "\n",
    "# ---EITHER--- Keyword search\n",
    "# keyword = 'data'\n",
    "# data = sntwitter.TwitterSearchScraper(keyword + '-filter:links -filter:replies').get_items()\n",
    "\n",
    "# ---OR--- User search\n",
    "#user = 'ASTRAZENECAUK'\n",
    "#data = sntwitter.TwitterSearchScraper('from:ASTRAZENECAUK').get_items()\n",
    "\n",
    "# ---OR--- Hastag search\n",
    "hashtag = '#AstraZeneca'\n",
    "data = sntwitter.TwitterHashtagScraper(hashtag + '-filter:links -filter:replies').get_items()\n",
    "\n",
    "#Set the label for this group\n",
    "hashtag_astrazeneca = []\n",
    "\n",
    "#Collect in array and pair each tweet with a class label\n",
    "for i, tweet in enumerate(data):\n",
    "    if i >= maxTweets:\n",
    "        break   \n",
    "    hashtag_astrazeneca = hashtag_astrazeneca + [[tweet.content]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2e1c1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "<class 'str'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"['üö® Atenci√≥n üö®\\\\n\\\\n¬°Solo por hoy!\\\\n\\\\nPara todos los mayores de 18 a√±os que hayan recibido su 2da dosis hace 4 meses podr√°n acercarse hoy a @BulevarCC @titanplazacc o Carrera a recibir su 3ra dosis de #AstraZeneca \\\\n\\\\nAprovecha la excepci√≥n y recibe tu dosis de refuerzo\\\\n\\\\nRTüê¶']\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(hashtag_astrazeneca))\n",
    "print(type(str(hashtag_astrazeneca[0]))) #make sure that the data type in the array are stings\n",
    "str(hashtag_astrazeneca[1]) #Check the tweets in the array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eece43c",
   "metadata": {},
   "source": [
    "### <p style=\"font-family:Garamond,serif\">I initially tried to use TextBlob to filter out English Tweets, however, the function has limitation on the amount of input data per-day, so I had to give up on this function.</p>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "09a9affe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from textblob import TextBlob, Word, Blobber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6da8c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U pip setuptools wheel\n",
    "!pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9d2d1a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "#intall spacy language detect to filer language\n",
    "!pip install spacy-langdetect "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d91da6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an array for storing English Tweets only\n",
    "en_hashtag_astrazeneca = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "252618cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import spacy for language detection\n",
    "#code sourse : https://stackoverflow.com/questions/66712753/how-to-use-languagedetector-from-spacy-langdetect-package\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy_langdetect import LanguageDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65f47393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy_langdetect.spacy_langdetect.LanguageDetector at 0x7f844b2510d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This bit of code only need to run once !\n",
    "def get_lang_detector(nlp, name):\n",
    "    return LanguageDetector()\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "Language.factory(\"language_detector\", func=get_lang_detector)\n",
    "nlp.add_pipe('language_detector', last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b90063a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'language': 'id', 'score': 0.9999967098958428}\n",
      "[['language' 'id']\n",
      " ['score' '0.9999980749772664']]\n"
     ]
    }
   ],
   "source": [
    "text = str(hashtag_astrazeneca[4])\n",
    "doc = nlp(text)\n",
    "print(doc._.language)\n",
    "type(doc._.language)\n",
    "\n",
    "#turn dict into array \n",
    "lang_doc = list(dict.items(doc._.language))\n",
    "lang_array = np.array(lang_doc)\n",
    "\n",
    "print(lang_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0dbb310d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(100000):\n",
    "    text = str(hashtag_astrazeneca[i])\n",
    "    doc = nlp(text)\n",
    "    doc._.language\n",
    "    lang_doc = list(dict.items(doc._.language))\n",
    "    lang_array = np.array(lang_doc)\n",
    "    #print(lang_array)\n",
    "    if lang_array[0][1] == 'en':\n",
    "        en_hashtag_astrazeneca.append(text)\n",
    "#print(en_lang)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "326fadd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['What would prevent #Pfizer,  #BioNTech, #J&amp;J and #AstraZeneca from changing up the formula of their VX to adjust its lethality and create specific types of variants?']\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_hashtag_astrazeneca[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "febf50ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23868, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_hashtag_astrazeneca_df = pd.DataFrame(en_hashtag_astrazeneca, columns = [\"text\"])\n",
    "en_hashtag_astrazeneca_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "115ed84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#>>>For cleaning dataframe if store wrong<<<\n",
    "#en_hashtag_pfizer_df.drop(en_hashtag_pfizer_df.index, inplace=True)\n",
    "#df_en_lang.drop(df_en_lang.loc[:80103].columns, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ac77cb5b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 981 kB 6.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /Users/chenxifang/opt/anaconda3/lib/python3.8/site-packages (from langdetect) (1.15.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993221 sha256=f3438c88683b2ad9e93ec1a9b22a6fdb99c43247b419a2a8258683be2e2cb08e\n",
      "  Stored in directory: /Users/chenxifang/Library/Caches/pip/wheels/13/c7/b0/79f66658626032e78fc1a83103690ef6797d551cb22e56e734\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.9\n"
     ]
    }
   ],
   "source": [
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed2d21cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[\"Those who got their vaccines like me in the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['#Pfizer , Johnson&amp;amp;Johnson , #Moderna , #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['Crytoverse and Metaverse face any risk on sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['What happened to the Astra Zeneca vaccine? #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['I am not anti vaccine or,as they insult a #C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23863</th>\n",
       "      <td>['This is a move that will come to bite these ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23864</th>\n",
       "      <td>['üá©üá™#Germany &amp;amp; üá®üáµ#France joining 13 other ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23865</th>\n",
       "      <td>['#AstraZeneca #Bundesgesundheitsminister plea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23866</th>\n",
       "      <td>['I would also like to hear from anyone that h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23867</th>\n",
       "      <td>['Morons, scared clueless incompetent morons. ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23868 rows √ó 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text\n",
       "0      [\"Those who got their vaccines like me in the ...\n",
       "1      ['#Pfizer , Johnson&amp;Johnson , #Moderna , #...\n",
       "2      ['Crytoverse and Metaverse face any risk on sh...\n",
       "3      ['What happened to the Astra Zeneca vaccine? #...\n",
       "4      ['I am not anti vaccine or,as they insult a #C...\n",
       "...                                                  ...\n",
       "23863  ['This is a move that will come to bite these ...\n",
       "23864  ['üá©üá™#Germany &amp; üá®üáµ#France joining 13 other ...\n",
       "23865  ['#AstraZeneca #Bundesgesundheitsminister plea...\n",
       "23866  ['I would also like to hear from anyone that h...\n",
       "23867  ['Morons, scared clueless incompetent morons. ...\n",
       "\n",
       "[23868 rows x 1 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_hashtag_astrazeneca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d19d8654",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write to file\n",
    "file_name = \"en_hashtag_astrazeneca\"\n",
    "en_hashtag_astrazeneca_df.to_csv(\"data/\" + file_name + \".tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2703f463",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read back in\n",
    "en_hashtag_astrazeneca= pd.read_csv(\"data/\" + file_name + \".tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bfcd3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
